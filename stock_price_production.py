# -*- coding: utf-8 -*-
"""stock price production

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HXOIKWQNyKfbdNww3PBAoO-4RysfH_KI
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

# Load the dataset
df=pd.read_excel("Stock Price Prediction.xlsx")

# Display the first few rows of the dataframe
print(df.head())

# Display basic statistics of the dataframe
print(df.describe())

# Display information about the dataframe
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Option 1: Drop rows with missing values
df.dropna(inplace=True)

# Option 2: Fill missing values (example: fill with mean)
df.fillna(df.mean(), inplace=True)

df.columns

# Drop columns that are not useful
df.drop(columns=['Volume'], inplace=True)

# Example: Convert date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Example: Extract day, month, and year from date
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year

# Summary statistics
print(df.describe())

import matplotlib.pyplot as plt

# Histogram of stock prices
df['Close'].hist(bins=50)
plt.xlabel('Stock Price')
plt.ylabel('Frequency')
plt.title('Distribution of Stock Prices')
plt.show()

# Scatter plot of Closing Price vs Volume
plt.scatter(df['Open'], df['Close'])
plt.xlabel('Volume')
plt.ylabel('Closing Price')
plt.title('Closing Price vs Volume')
plt.show()

import seaborn as sns

# Correlation matrix
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

X = df.drop(columns=['Close'])
y = df['Close']

# Convert date column to datetime
X['Date'] = pd.to_datetime(X['Date'])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert datetime column to numerical features for training set
X_train['year'] = X_train['Date'].dt.year
X_train['month'] = X_train['Date'].dt.month
X_train['day'] = X_train['Date'].dt.day

# Convert datetime column to numerical features for test set
X_test['year'] = X_test['Date'].dt.year
X_test['month'] = X_test['Date'].dt.month
X_test['day'] = X_test['Date'].dt.day

# Drop the original Date column
X_train = X_train.drop('Date', axis=1)
X_test = X_test.drop('Date', axis=1)

# Ensure all columns are numeric
print(X_train.dtypes)
print(X_test.dtypes)

# Apply StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Display first few rows of the scaled data
print(pd.DataFrame(X_train_scaled).head())
print(pd.DataFrame(X_test_scaled).head())

# example for RandomForest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='r2')
grid_search.fit(X_train_scaled, y_train)

# Best parameters
print("Best parameters for Random Forest:", grid_search.best_params_)

# Train with best parameters
best_rf = grid_search.best_estimator_
best_rf.fit(X_train_scaled, y_train)

# Predictions
best_rf_pred = best_rf.predict(X_test_scaled)

# Evaluate best model
print("Best Random Forest R2 Score:", r2_score(y_test, best_rf_pred))
print("Best Random Forest MSE:", mean_squared_error(y_test, best_rf_pred))

#Save model
import pickle

with open('stock price prediction_model.pkl', 'wb') as file:
    pickle.dump(df, file)